#include "vmm.h"
#include "memdefs.h"
#include <libk/kassert.h>
#include <amd64/validity.h>
#include <drivers/gfx/gfx.h>
#include <int/gdt.h>
#include <int/idt.h>
#include <util/ptr.h>
#include <stdbool.h>

#define GB 0x40000000UL

// struct page_directory pml4;
static uint64_t *pml4;

void vmm_init()
{
    init_gdt();
    init_idt();

    pml4 = (uint64_t *)pmm_alloc();
    printk("vmm", "pml4 resides at 0x%llx\n", pml4);

    // Identity map first 4 GB
    for (size_t n = 0; n < 4 * GB; n += PAGE_SIZE)
    {
        vmm_map(to_phys(n), n, FLAGS_PR | FLAGS_RW);
    }

    for (size_t n = 0; n < 4 * GB; n += PAGE_SIZE)
    {
        vmm_map(to_virt(n), n, FLAGS_PR | FLAGS_RW);
    }

    // Map framebuffer
    struct memtag_range framebuffer = pmm_find_tag(STIVALE2_MMAP_FRAMEBUFFER, 1);
    for (size_t n = 0; n < framebuffer.size; n += PAGE_SIZE)
        vmm_map(n, n, FLAGS_PR | FLAGS_RW);

    debug(true, "Old PML4: %llx\n", cr_read(CR3)); // Bootloader pml4
    PAGE_LOAD_CR3(GENERIC_CAST(uint64_t, pml4));
    debug(true, "New PML4: %llx\n", cr_read(CR3)); // Kernel pml4

    printk("vmm", "Initialised vmm\n");
}

static uint64_t *vmm_get_pml_or_alloc(uint64_t *entry, size_t level, int flags)
{
    if (entry[level] & 1)
        goto no_alloc;

    entry[level] = VOID_PTR_TO_64(pmm_alloc());
    entry[level] |= flags;

    no_alloc:
    return GENERIC_CAST(uint64_t*, (entry[level] & ~(511)));
}

static uint64_t *vmm_get_pml(uint64_t *entry, size_t level)
{
    return GENERIC_CAST(uint64_t*, (entry[level] & ~(511)));
}

void vmm_map(size_t vaddr, size_t paddr, int flags)
{
    //TODO:
    //Performance tweak: Use an algorithm to save an address
    //which has been mapped already and check if the address is already mapped here
    page_info_t info = vmm_dissect_vaddr(vaddr);

    uint64_t *pml3, *pml2, *pml1 = NULL;
    pml3 = vmm_get_pml_or_alloc(pml4, info.lv4, flags);
    pml2 = vmm_get_pml_or_alloc(pml3, info.lv3, flags);
    pml1 = vmm_get_pml_or_alloc(pml2, info.lv2, flags);
    pml1[info.lv1] = (paddr | flags);
}

//Todo: This does not unmap pages, why?
void vmm_unmap(size_t vaddr)
{
    page_info_t info = vmm_dissect_vaddr(vaddr);

    uint64_t *pml3, *pml2, *pml1 = NULL;
    pml3 = vmm_get_pml(pml4, info.lv4);
    pml2 = vmm_get_pml(pml3, info.lv3);
    pml1 = vmm_get_pml(pml2, info.lv2);
    pml1[info.lv1] = 0;

    __asm__ volatile("invlpg (%0)" :: "r"(vaddr) : "memory");
}

//Pagefault handler
void vmm_guess_and_map(uint64_t cr2, int error_code)
{
    /* Non present page */
    if (error_code & 1)
        vmm_map(cr2, cr2, (error_code & 2) ? (FLAGS_PR | FLAGS_RW) : FLAGS_PR);

    /* CPL = 3 */
    else if (error_code & 4)
        ;

    /* Page fault generated by ab Instruction fetch, only when NX is enabled (Always will be because of limine) */
    else if (error_code & 16)
        ;

    else
        vmm_map(cr2, cr2, FLAGS_PR | FLAGS_RW);
}

page_info_t vmm_dissect_vaddr(uint64_t virt_addr)
{
    page_info_t pg_info;
    const int bitmask = 0x1FF;

    virt_addr >>= 12;

    pg_info.lv1 = virt_addr & bitmask;
    virt_addr >>= 9;

    pg_info.lv2 = virt_addr & bitmask;
    virt_addr >>= 9;

    pg_info.lv3 = virt_addr & bitmask;
    virt_addr >>= 9;

    pg_info.lv4 = virt_addr & bitmask;

    return pg_info;
}
